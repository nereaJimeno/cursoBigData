{"cells":[{"cell_type":"markdown","metadata":{"id":"G0LOaPyAgCU8"},"source":["#  RDDs "]},{"cell_type":"markdown","metadata":{"id":"yxqIrsY-gCVB"},"source":["# Creamos un contexto para crear RDDs"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"T3SkkrkIgCVD","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1681289720231,"user_tz":-120,"elapsed":52072,"user":{"displayName":"Nerea Jimeno Noriega","userId":"09762042781573971718"}},"outputId":"d141fc1f-b566-45d6-bea9-f58183111cc3"},"outputs":[{"output_type":"stream","name":"stdout","text":["\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m281.4/281.4 MB\u001b[0m \u001b[31m3.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m199.7/199.7 kB\u001b[0m \u001b[31m13.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25h  Building wheel for pyspark (setup.py) ... \u001b[?25l\u001b[?25hdone\n"]}],"source":["!pip install pyspark --quiet\n","from pyspark import SparkContext"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"_X-TkHs-gCVE"},"outputs":[],"source":["sc = SparkContext(master = \"local\", appName=\"TransformacionesyAcciones\")"]},{"cell_type":"markdown","metadata":{"id":"oYNeSv5EgCVF"},"source":["# Un RDD es una colección inmutable y distribuida de elementos\n","\n","### Spark automaticamente distribuye los datos y paraleliza las operaciones\n","\n","### Los RDD realmente cargan colecciones de datos"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"-yfNRIqPgCVF"},"outputs":[],"source":["rdd1 = sc.parallelize([1,2,3])"]},{"cell_type":"markdown","metadata":{"id":"7daw3WsugCVF"},"source":["### Debido a la propiedad de distribución que tienen los RDD, en su creación, podemos particionar los datos"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"gV_9CtH-gCVG"},"outputs":[],"source":["import numpy as np\n","rdd2 = sc.parallelize(np.array(range(100)),2)"]},{"cell_type":"markdown","metadata":{"id":"OXf9xyz4gCVG"},"source":["### Verificamos el tipo de dato"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"-6IXNzw2gCVG","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1681289786254,"user_tz":-120,"elapsed":593,"user":{"displayName":"Nerea Jimeno Noriega","userId":"09762042781573971718"}},"outputId":"ad980b04-4184-4027-db8d-b68d3331fb5f"},"outputs":[{"output_type":"execute_result","data":{"text/plain":["pyspark.rdd.RDD"]},"metadata":{},"execution_count":6}],"source":["type(rdd1)"]},{"cell_type":"markdown","metadata":{"id":"M9fdoJ87gCVG"},"source":["### Vemos el contenido\n","\n","Veremos distintas tecnicas apropiadas para ver el contenido de los RDDs y Dataframes"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"QzqUUjhCgCVH","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1681289791991,"user_tz":-120,"elapsed":1143,"user":{"displayName":"Nerea Jimeno Noriega","userId":"09762042781573971718"}},"outputId":"3f7df519-3557-4dbb-ed23-d0dcc5d0e93b"},"outputs":[{"output_type":"execute_result","data":{"text/plain":["[1, 2, 3]"]},"metadata":{},"execution_count":7}],"source":["rdd1.collect()"]},{"cell_type":"markdown","metadata":{"id":"cvXwqRwdgCVH"},"source":["# Carga de un arhivo CSV"]},{"cell_type":"markdown","metadata":{"id":"OfQ67xw1gCVI"},"source":["### Cargamos un RDDs\n","\n","El método textFile busca el archivo en la ruta indicada\n","\n","Cambia el valor de la ruta para que apunte a la ruta donde tienes los datos"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"9AG-1zOcgCVI","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1681289820950,"user_tz":-120,"elapsed":22925,"user":{"displayName":"Nerea Jimeno Noriega","userId":"09762042781573971718"}},"outputId":"b8439aab-b59f-48b9-fcaf-bb7d83848efb"},"outputs":[{"output_type":"stream","name":"stdout","text":["Mounted at /content/drive/\n"]}],"source":["from google.colab import drive\n","drive.mount('/content/drive/')\n","\n","equiposOlimpicosRDD = sc.textFile(\"/content/drive/MyDrive/Colab Notebooks/RED.ES/Datos Ejercicios/M6/paises.csv\",2).map(lambda line : line.split(\",\"))"]},{"cell_type":"markdown","metadata":{"id":"FGKXqN4sgCVI"},"source":["### Vemos el contenido\n","\n","El método take es otro método existnte para poder visualizar el contenido de los RDDs"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"HA3o7cq1gCVI","colab":{"base_uri":"https://localhost:8080/","height":831},"executionInfo":{"status":"error","timestamp":1681289832417,"user_tz":-120,"elapsed":269,"user":{"displayName":"Nerea Jimeno Noriega","userId":"09762042781573971718"}},"outputId":"a22ec6cc-8882-47ef-dc64-d6663cd16b65"},"outputs":[{"output_type":"error","ename":"Py4JJavaError","evalue":"ignored","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mPy4JJavaError\u001b[0m                             Traceback (most recent call last)","\u001b[0;32m<ipython-input-9-8ff8629e2858>\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mequiposOlimpicosRDD\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtake\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m10\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m","\u001b[0;32m/usr/local/lib/python3.9/dist-packages/pyspark/rdd.py\u001b[0m in \u001b[0;36mtake\u001b[0;34m(self, num)\u001b[0m\n\u001b[1;32m   1848\u001b[0m         \"\"\"\n\u001b[1;32m   1849\u001b[0m         \u001b[0mitems\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mList\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mT\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1850\u001b[0;31m         \u001b[0mtotalParts\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgetNumPartitions\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1851\u001b[0m         \u001b[0mpartsScanned\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1852\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.9/dist-packages/pyspark/rdd.py\u001b[0m in \u001b[0;36mgetNumPartitions\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   3489\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3490\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mgetNumPartitions\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mint\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3491\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_prev_jrdd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpartitions\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   3492\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3493\u001b[0m     \u001b[0;34m@\u001b[0m\u001b[0mproperty\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.9/dist-packages/py4j/java_gateway.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m   1319\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1320\u001b[0m         \u001b[0manswer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgateway_client\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msend_command\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcommand\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1321\u001b[0;31m         return_value = get_return_value(\n\u001b[0m\u001b[1;32m   1322\u001b[0m             answer, self.gateway_client, self.target_id, self.name)\n\u001b[1;32m   1323\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.9/dist-packages/py4j/protocol.py\u001b[0m in \u001b[0;36mget_return_value\u001b[0;34m(answer, gateway_client, target_id, name)\u001b[0m\n\u001b[1;32m    324\u001b[0m             \u001b[0mvalue\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mOUTPUT_CONVERTER\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mtype\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0manswer\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgateway_client\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    325\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0manswer\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mREFERENCE_TYPE\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 326\u001b[0;31m                 raise Py4JJavaError(\n\u001b[0m\u001b[1;32m    327\u001b[0m                     \u001b[0;34m\"An error occurred while calling {0}{1}{2}.\\n\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    328\u001b[0m                     format(target_id, \".\", name), value)\n","\u001b[0;31mPy4JJavaError\u001b[0m: An error occurred while calling o27.partitions.\n: org.apache.hadoop.mapred.InvalidInputException: Input path does not exist: file:/content/drive/MyDrive/Colab Notebooks/RED.ES/Datos Ejercicios/M6/paises.csv\n\tat org.apache.hadoop.mapred.FileInputFormat.singleThreadedListStatus(FileInputFormat.java:304)\n\tat org.apache.hadoop.mapred.FileInputFormat.listStatus(FileInputFormat.java:244)\n\tat org.apache.hadoop.mapred.FileInputFormat.getSplits(FileInputFormat.java:332)\n\tat org.apache.spark.rdd.HadoopRDD.getPartitions(HadoopRDD.scala:208)\n\tat org.apache.spark.rdd.RDD.$anonfun$partitions$2(RDD.scala:292)\n\tat scala.Option.getOrElse(Option.scala:189)\n\tat org.apache.spark.rdd.RDD.partitions(RDD.scala:288)\n\tat org.apache.spark.rdd.MapPartitionsRDD.getPartitions(MapPartitionsRDD.scala:49)\n\tat org.apache.spark.rdd.RDD.$anonfun$partitions$2(RDD.scala:292)\n\tat scala.Option.getOrElse(Option.scala:189)\n\tat org.apache.spark.rdd.RDD.partitions(RDD.scala:288)\n\tat org.apache.spark.api.java.JavaRDDLike.partitions(JavaRDDLike.scala:61)\n\tat org.apache.spark.api.java.JavaRDDLike.partitions$(JavaRDDLike.scala:61)\n\tat org.apache.spark.api.java.AbstractJavaRDDLike.partitions(JavaRDDLike.scala:45)\n\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\n\tat java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.base/java.lang.reflect.Method.invoke(Method.java:566)\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\n\tat py4j.Gateway.invoke(Gateway.java:282)\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n\tat py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)\n\tat py4j.ClientServerConnection.run(ClientServerConnection.java:106)\n\tat java.base/java.lang.Thread.run(Thread.java:829)\nCaused by: java.io.IOException: Input path does not exist: file:/content/drive/MyDrive/Colab Notebooks/RED.ES/Datos Ejercicios/M6/paises.csv\n\tat org.apache.hadoop.mapred.FileInputFormat.singleThreadedListStatus(FileInputFormat.java:278)\n\t... 25 more\n"]}],"source":["equiposOlimpicosRDD.take(10)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"LfOUKBkvgCVJ"},"outputs":[],"source":["sc.stop()"]},{"cell_type":"markdown","source":["## Ejercicio:\n","\n","Intenta leer un archivo csv o de texto y hacer alguna transformación.\n","\n","* Busca el archivo del quijote.txt por internet. \n","* Subelo a Google COlab e importalo con pyspark. \n","* Cuenta los caracteres del fichero."],"metadata":{"id":"a0DSWeq2xcEH"}},{"cell_type":"code","source":["#Inserta aquí tú código\n","\n","#primero cargaremos y leeremos el documento llamado quijote.txt\n","from google.colab import drive\n","drive.mount('/content/drive/')\n","\n","quijote = sc.textFile('/content/drive/MyDrive/Colab Notebooks/quijote.txt')\n","\n","#Después procedemos a crear la variable num_caracteres para contarlos con .reduce()\n","num_caracteres = quijote.map(lambda x: len(list(x))).reduce(lambda x, y: x + y)\n","\n","#Por último imprimimos por pantalla el num_caracteres\n","print('El número total de caracteres en el archivo es:', num_caracteres)\n","\n"],"metadata":{"id":"vSx9LCyjxo9f","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1681296175569,"user_tz":-120,"elapsed":2506,"user":{"displayName":"Nerea Jimeno Noriega","userId":"09762042781573971718"}},"outputId":"8c1e049a-ceeb-4acf-f878-74fae57d1114"},"execution_count":16,"outputs":[{"output_type":"stream","name":"stdout","text":["Drive already mounted at /content/drive/; to attempt to forcibly remount, call drive.mount(\"/content/drive/\", force_remount=True).\n","El número total de caracteres en el archivo es: 2092335\n"]}]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.8.5"},"colab":{"provenance":[{"file_id":"1bB5s6wfR7BmWZitfFkhiFLH0YhleTOsP","timestamp":1681289505408},{"file_id":"https://github.com/terranigmark/curso-apache-spark-platzi/blob/master/2.%20Primer%20RDD.ipynb","timestamp":1672143190353}]}},"nbformat":4,"nbformat_minor":0}